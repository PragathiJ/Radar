{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:02.844116Z",
     "iopub.status.busy": "2021-09-14T14:28:02.843280Z",
     "iopub.status.idle": "2021-09-14T14:28:07.910763Z",
     "shell.execute_reply": "2021-09-14T14:28:07.910286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "2.5.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os \n",
    "from random import shuffle\n",
    "import cv2\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:07.914021Z",
     "iopub.status.busy": "2021-09-14T14:28:07.913686Z",
     "iopub.status.idle": "2021-09-14T14:28:07.915327Z",
     "shell.execute_reply": "2021-09-14T14:28:07.915056Z"
    }
   },
   "outputs": [],
   "source": [
    "sampling_rate = 1024\n",
    "gesture_size = 1536 #each gesture records 1.5 seconds\n",
    "time_ms = (1/sampling_rate) * 1000 #sampling time in ms\n",
    "nfft = 64 #window length -> 256 samples - 125 ms and 128 samples - 62.5ms\n",
    "overlap = 58 #87.5% overlapping [128 - 112ms and 256 - 224ms]\n",
    "zp =  0\n",
    "T_sample = 1.5 #in seconds\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "gesture_mapping = {'double_tap': 4,\n",
    "                   'tick': 3,\n",
    "                   'swipe': 2,\n",
    "                   'up_to_down': 1,\n",
    "                   'nothing': 0\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:07.918856Z",
     "iopub.status.busy": "2021-09-14T14:28:07.918524Z",
     "iopub.status.idle": "2021-09-14T14:28:07.942486Z",
     "shell.execute_reply": "2021-09-14T14:28:07.942194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 47)\n",
      "<class 'numpy.uint8'>\n",
      "32\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "path = r'/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/ml_radar/Radar/Data/Images/1024_64nfft/Grayscale_rfft/Subject1'     \n",
    "#path = r'/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/radar/1024_64nfft/Grayscale_rfft/Subject1'              \n",
    "image_paths = os.listdir(path)\n",
    "shuffle(image_paths)\n",
    "#print(image_paths)\n",
    "\n",
    "path2 = r'/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/ml_radar/Radar/Data/Images/1024_64nfft/Grayscale_rfft/Subject2'    \n",
    "#path2 = r'/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/radar/1024_64nfft/Grayscale_rfft/Subject2'               \n",
    "image_paths2 = os.listdir(path2)\n",
    "shuffle(image_paths2)\n",
    "\n",
    "path1 = r'/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/ml_radar/Radar/Data/Images/1024_64nfft/Grayscale_rfft/Test'   \n",
    "#path1 = r'/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/radar/1024_64nfft/Grayscale_rfft/Test'                \n",
    "image_paths1 = os.listdir(path1)\n",
    "shuffle(image_paths1)\n",
    "\n",
    "img_path1 = os.path.join(path, 'double_tap-11_rfft.png')\n",
    "image1 = cv2.imread(img_path1)\n",
    "image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "print(image1.shape)\n",
    "print(type(image1[0][0]))\n",
    "\n",
    "IMAGE_HEIGHT = int(image1.shape[0])\n",
    "IMAGE_WIDTH = int(image1.shape[1]) #47\n",
    "print(IMAGE_HEIGHT)\n",
    "print(IMAGE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:07.945082Z",
     "iopub.status.busy": "2021-09-14T14:28:07.944755Z",
     "iopub.status.idle": "2021-09-14T14:28:07.946094Z",
     "shell.execute_reply": "2021-09-14T14:28:07.946367Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_int(word_label):\n",
    "    label = gesture_mapping[word_label]\n",
    "    label_arr = np.zeros(5)\n",
    "    label_arr[label] = 1\n",
    "    return label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:07.951341Z",
     "iopub.status.busy": "2021-09-14T14:28:07.950982Z",
     "iopub.status.idle": "2021-09-14T14:28:10.186130Z",
     "shell.execute_reply": "2021-09-14T14:28:10.186409Z"
    }
   },
   "outputs": [],
   "source": [
    "for img in image_paths:\n",
    "    word_label = img.split('-')[0]\n",
    "    label = label_int(word_label)\n",
    "    img_path = os.path.join(path, img)\n",
    "    #image = PIL.Image.open(img_path)\n",
    "    image = cv2.imread(img_path)\n",
    "    #image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    #image = image.astype('float') / 255.0\n",
    "    train_data.append([image, label])\n",
    "\n",
    "for img2 in image_paths2:\n",
    "    word_label2 = img2.split('-')[0]\n",
    "    label2 = label_int(word_label2)\n",
    "    img_path2 = os.path.join(path2, img2)\n",
    "    #image = PIL.Image.open(img_path)\n",
    "    image2 = cv2.imread(img_path2)\n",
    "    #image2 = cv2.resize(image2, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    #image = image.astype('float') / 255.0\n",
    "    train_data.append([image2, label2])\n",
    "\n",
    "for img_t in image_paths1:\n",
    "    word_label_t = img_t.split('-')[0]\n",
    "    label_t = label_int(word_label_t)\n",
    "    img_path1 = os.path.join(path1, img_t)\n",
    "    #image_t = PIL.Image.open(img_path1)\n",
    "    image_t = cv2.imread(img_path1)\n",
    "    #image_t = cv2.resize(image_t, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    image_t = cv2.cvtColor(image_t, cv2.COLOR_BGR2GRAY)\n",
    "    #image_t = image_t.astype('float') / 255.0\n",
    "    test_data.append([image_t, label_t])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:10.192666Z",
     "iopub.status.busy": "2021-09-14T14:28:10.192302Z",
     "iopub.status.idle": "2021-09-14T14:28:10.209985Z",
     "shell.execute_reply": "2021-09-14T14:28:10.210243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of images = 2000\n",
      "input image shape = (32, 47)\n",
      "number of training + validation examples = 1900\n",
      "number of test examples = 100\n",
      "X_train shape: (1615, 32, 47)\n",
      "Y_train shape: (1615, 5)\n",
      "X_val shape: (285, 32, 47)\n",
      "Y_val shape: (285, 5)\n",
      "X_test shape: (100, 32, 47)\n",
      "Y_test shape: (100, 5)\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "data_size = len(train_data)+len(test_data)\n",
    "input_shape = image.shape  \n",
    "split_ratio = 0.8\n",
    "train_size = int(data_size * split_ratio)\n",
    "NUM_CLASSES = 5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.8\n",
    "DECAY = 1e-06\n",
    "\n",
    "#print(train_data[0])\n",
    "print(\"total number of images = \" + str(data_size))\n",
    "print(\"input image shape = \" + str(input_shape))\n",
    "\n",
    "#train_data = data[:train_size]\n",
    "train_images = np.array([i[0] for i in train_data])\n",
    "#.reshape(-1, IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "train_labels = np.array([i[1] for i in train_data])\n",
    "\n",
    "#test_data = data[train_size:]\n",
    "X_test = np.array([i[0] for i in test_data])\n",
    "#.reshape(-1, IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "Y_test = np.array([i[1] for i in test_data])\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_images, \n",
    "                                                  train_labels, \n",
    "                                                  test_size=0.15, \n",
    "                                                  stratify=np.array(train_labels), \n",
    "                                                  random_state=42)\n",
    "\n",
    "\n",
    "print (\"number of training + validation examples = \" + str(train_images.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_val shape: \" + str(X_val.shape))\n",
    "print (\"Y_val shape: \" + str(Y_val.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:10.225292Z",
     "iopub.status.busy": "2021-09-14T14:28:10.224948Z",
     "iopub.status.idle": "2021-09-14T14:28:10.259522Z",
     "shell.execute_reply": "2021-09-14T14:28:10.259796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.imshow(data[15][0])\n",
    "#plt.axis('off')\n",
    "#print(data[15][1])\n",
    "\n",
    "\n",
    "plt.imshow(X_train[1])\n",
    "plt.axis('off')\n",
    "print(Y_train[1])\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "#print(X_train[1].reshape(1, IMAGE_HEIGHT, IMAGE_WIDTH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:10.266160Z",
     "iopub.status.busy": "2021-09-14T14:28:10.265808Z",
     "iopub.status.idle": "2021-09-14T14:28:10.267464Z",
     "shell.execute_reply": "2021-09-14T14:28:10.267184Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.Sequential()#Sequential() #4-6-4-6 #4-6-8-4 #4-8-6-4 #4-6-6-4 \n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Activation(None, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(4, kernel_size = (3, 3), activation='relu', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(6, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(8, kernel_size=(3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dropout(0.2)) \n",
    "    model.add(Dense(5, activation = 'softmax', kernel_regularizer=regularizers.l2(0.0001), activity_regularizer=regularizers.l2(0.0001)))\n",
    "\n",
    "    #activation = 'softmax', kernel_regularizer=regularizers.l2(0.001), activity_regularizer=regularizers.l2(0.001)\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE, decay=DECAY, momentum=MOMENTUM, nesterov=True)\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    #print('model prepared...')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:10.269683Z",
     "iopub.status.busy": "2021-09-14T14:28:10.269318Z",
     "iopub.status.idle": "2021-09-14T14:28:10.372457Z",
     "shell.execute_reply": "2021-09-14T14:28:10.372108Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:28:10.375453Z",
     "iopub.status.busy": "2021-09-14T14:28:10.374879Z",
     "iopub.status.idle": "2021-09-14T14:30:34.667529Z",
     "shell.execute_reply": "2021-09-14T14:30:34.667248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "51/51 [==============================] - 2s 25ms/step - loss: 1.5198 - accuracy: 0.3659 - val_loss: 1.6192 - val_accuracy: 0.2491\n",
      "Epoch 2/2\n",
      "51/51 [==============================] - 1s 21ms/step - loss: 1.0258 - accuracy: 0.6762 - val_loss: 1.4722 - val_accuracy: 0.3368\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:30:34.671469Z",
     "iopub.status.busy": "2021-09-14T14:30:34.670919Z",
     "iopub.status.idle": "2021-09-14T14:30:34.811935Z",
     "shell.execute_reply": "2021-09-14T14:30:34.811624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:30:34.815755Z",
     "iopub.status.busy": "2021-09-14T14:30:34.815217Z",
     "iopub.status.idle": "2021-09-14T14:30:34.941559Z",
     "shell.execute_reply": "2021-09-14T14:30:34.941847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 1.4996 - accuracy: 0.4000\n",
      "Loss = 1.4995859861373901\n",
      "Model has an accuracy of 40.00%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(predictions[0]))\n",
    "#print (\"Test Accuracy = \" + str(predictions[1]))\n",
    "print(\"Model has an accuracy of {0:.2f}%\".format(predictions[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:30:34.944981Z",
     "iopub.status.busy": "2021-09-14T14:30:34.944641Z",
     "iopub.status.idle": "2021-09-14T14:30:35.082705Z",
     "shell.execute_reply": "2021-09-14T14:30:35.082987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0]\n",
      " [ 3  0 16  1  0]\n",
      " [ 6  0 14  0  0]\n",
      " [16  0  4  0  0]\n",
      " [ 0  0  8  6  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tt= model.predict(X_test)\n",
    "cm=confusion_matrix(np.argmax(Y_test,axis=1), np.argmax(tt,axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:30:35.086218Z",
     "iopub.status.busy": "2021-09-14T14:30:35.085827Z",
     "iopub.status.idle": "2021-09-14T14:30:35.158025Z",
     "shell.execute_reply": "2021-09-14T14:30:35.157679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 47)\n",
      "[[0.19357474 0.09252723 0.19843073 0.22408181 0.29138553]]\n",
      "[0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 49\n",
    "print(X_test[index].shape)\n",
    "xi = X_test[index].reshape(1, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "#print(xi.shape)\n",
    "#print(X_test.shape)\n",
    "\n",
    "tt= model.predict(xi)\n",
    "print(tt)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.resize(X_test[index], (IMAGE_WIDTH, IMAGE_HEIGHT)))\n",
    "print(Y_test[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:30:35.161758Z",
     "iopub.status.busy": "2021-09-14T14:30:35.160981Z",
     "iopub.status.idle": "2021-09-14T14:30:35.164986Z",
     "shell.execute_reply": "2021-09-14T14:30:35.165265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "activation_14 (Activation)   (None, 32, 47, 1)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 32, 47, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 30, 45, 4)         40        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 15, 22, 4)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 15, 22, 4)         16        \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 13, 20, 6)         222       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 6, 10, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 6, 10, 6)          24        \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 4, 8, 8)           440       \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 4, 8, 8)           32        \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 2, 6, 4)           292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 1, 3, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 1, 3, 4)           16        \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                416       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,667\n",
      "Trainable params: 1,621\n",
      "Non-trainable params: 46\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-14T14:30:35.172624Z",
     "iopub.status.busy": "2021-09-14T14:30:35.172244Z",
     "iopub.status.idle": "2021-09-14T14:30:35.208142Z",
     "shell.execute_reply": "2021-09-14T14:30:35.207852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4_g2eoxv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4_g2eoxv/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpeq6ol05d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpeq6ol05d/assets\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset_gen():\n",
    "  for image in X_test:\n",
    "    image = image.reshape(1, IMAGE_HEIGHT, IMAGE_WIDTH,1)\n",
    "    image = image.astype('float32')\n",
    "    yield [image]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "#converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "#open(\"radar_gesture.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "#import os\n",
    "#basic_model_size = os.path.getsize(\"radar_gesture.tflite\")\n",
    "#print(\"Model is %d bytes\" % basic_model_size)\n",
    "\n",
    "#open(\"radar_gesture.tflite\", \"wb\").write(tflite_model)\n",
    "#!xxd -i radar_gesture.tflite > radar_gesture.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.int8'>\n",
      "output:  <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13792"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/data/radar_gesture\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"radar_gesture.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Save the quantized model:\n",
    "#tflite_model_quant_file = tflite_models_dir/\"radar_gesture.tflite\"\n",
    "#tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test1 = []\n",
    "for img_t in image_paths1:\n",
    "    word_label_t = img_t.split('-')[0]\n",
    "    label = gesture_mapping[word_label_t]\n",
    "    Y_test1.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  #global test_images\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = X_test[test_image_index].reshape( IMAGE_HEIGHT, IMAGE_WIDTH, 1)\n",
    "    test_label = Y_test1[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.int8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  #global X_test\n",
    "  #global Y_test\n",
    "\n",
    "  test_image_indices = range(X_test.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(Y_test1== predictions) * 100) / len(X_test)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 40.0000% (Number of test samples=100)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tflite_model_quant_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-4a56cb1379c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model_quant_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Quantized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tflite_model_quant_file' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")\n",
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "   \n",
    "#print(Y_test1)    \n",
    "# Change this to test a different image\n",
    "test_image_index = 40\n",
    "\n",
    "## Helper function to test the models on one image\n",
    "def test_model(tflite_file, test_image_index, model_type):\n",
    "  global test_labels\n",
    "\n",
    "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
    "\n",
    "  print(predictions)\n",
    "  plt.imshow(X_test[test_image_index])\n",
    "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
    "  _ = plt.title(template.format(true= str(Y_test1[test_image_index]), predict=str(predictions[0])))\n",
    "  plt.grid(False)\n",
    "\n",
    "\n",
    "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 32, 47, 1)         3         \n",
      "_________________________________________________________________\n",
      "quant_activation_2 (Quantize (None, 32, 47, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_8 (QuantizeWrap (None, 30, 45, 4)         51        \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_6 (Quant (None, 15, 22, 4)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_9 (QuantizeWrap (None, 13, 20, 6)         237       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_7 (Quant (None, 6, 10, 6)          1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_10 (QuantizeWra (None, 4, 8, 8)           459       \n",
      "_________________________________________________________________\n",
      "quant_conv2d_11 (QuantizeWra (None, 2, 6, 4)           303       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_8 (Quant (None, 1, 3, 4)           1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_2 (QuantizeWra (None, 1, 3, 4)           1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_2 (QuantizeWra (None, 12)                1         \n",
      "_________________________________________________________________\n",
      "quant_dense_4 (QuantizeWrapp (None, 32)                421       \n",
      "_________________________________________________________________\n",
      "quant_dense_5 (QuantizeWrapp (None, 5)                 170       \n",
      "=================================================================\n",
      "Total params: 1,650\n",
      "Trainable params: 1,575\n",
      "Non-trainable params: 75\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(loss ='categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n",
    "\n",
    "#history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: dense_5/ActivityRegularizer/truediv:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-35d882c4036d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_aware_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: dense_5/ActivityRegularizer/truediv:0"
     ]
    }
   ],
   "source": [
    "#history = q_aware_model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 1, validation_data=(X_val,Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ge73pal/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4930: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: dense_9/ActivityRegularizer/truediv:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-fddfcd0b9a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m q_aware_model_accuracy = q_aware_model.evaluate(\n\u001b[0;32m----> 5\u001b[0;31m    X_test, Y_test, verbose=0)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_model_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 957\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis_projectgit/ml/ml_on_mcu/venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: dense_9/ActivityRegularizer/truediv:0"
     ]
    }
   ],
   "source": [
    "baseline_model_accuracy = model.evaluate(\n",
    "    X_test, Y_test, verbose=0)\n",
    "\n",
    "q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model1(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(X_test):\n",
    "    test_image = test_image.reshape( IMAGE_HEIGHT, IMAGE_WIDTH, 1)\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == Y_test1).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model1(interpreter)\n",
    "\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59ec3b56c67a2e60319da0016650e72c65c844b1c826c38f226e2dc072578c35"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
